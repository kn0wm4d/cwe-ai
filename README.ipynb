{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CWE AI\n",
        "\n",
        "This code is used to build a machine learning model that can predict the category of a software vulnerability based on its name. The categories are called CWEs, and they are assigned a number.\n",
        "\n",
        "The dataset (tagged_cve_cwe.tsv) was built by scraping Snyk Database and the official NIST CVE Data Feeds: \n",
        "- Titles of Vulnerabilities gathered from Snyk Database\n",
        "- CVE from the Vulnerability\n",
        "- NVD Descriptions are also on a column in case you want to use them.\n",
        "- CWE that matches the CVE (in case there are many, we use the first one)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk scikit-learn pandas"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: nltk in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (3.8.1)\nRequirement already satisfied: scikit-learn in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (1.2.2)\nRequirement already satisfied: pandas in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (1.5.3)\nRequirement already satisfied: joblib in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from nltk) (1.2.0)\nRequirement already satisfied: click in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from nltk) (8.1.3)\nRequirement already satisfied: tqdm in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from nltk) (4.65.0)\nRequirement already satisfied: regex>=2021.8.3 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from nltk) (2023.3.23)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\nRequirement already satisfied: numpy>=1.17.3 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from scikit-learn) (1.23.5)\nRequirement already satisfied: scipy>=1.3.2 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from scikit-learn) (1.5.3)\nRequirement already satisfied: python-dateutil>=2.8.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from pandas) (2022.5)\nRequirement already satisfied: six>=1.5 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679618044116
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679617662669
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above lines import the necessary libraries required to perform text preprocessing and build a machine learning model for classification."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_prepare(text):\n",
        "    \"\"\"Performs tokenization and simple preprocessing.\"\"\"\n",
        "    replace_by_space_re = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "    bad_symbols_re = re.compile('[^0-9a-z #+_]')\n",
        "    stopwords_set = set(stopwords.words('english'))\n",
        "    text = text.lower()\n",
        "    text = replace_by_space_re.sub(' ', text)\n",
        "    text = bad_symbols_re.sub('', text)\n",
        "    text = ' '.join([x for x in text.split() if x and x not in stopwords_set])\n",
        "    return text.strip()"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679617664725
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text_prepare function performs text preprocessing, which involves converting all text to lowercase, replacing certain characters with spaces, removing certain symbols, removing stopwords (common words such as \"the\", \"a\", \"an\"), and returning the cleaned text."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tfidf_features(X_train, X_test, vectorizer_path):\n",
        "    \"\"\"Performs TF-IDF transformation and dumps the model.\"\"\"\n",
        "    tfidf_vectorizer = TfidfVectorizer(min_df=5, max_df=0.9, ngram_range=(1, 2), token_pattern='(\\S+)')\n",
        "    X_train=tfidf_vectorizer.fit_transform(X_train)\n",
        "    X_test=tfidf_vectorizer.transform(X_test)\n",
        "    with open(vectorizer_path,'wb') as vectorizer_file:\n",
        "        pickle.dump(tfidf_vectorizer,vectorizer_file)\n",
        "    return X_train, X_test"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679617666678
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tfidf_features function performs the TF-IDF transformation on the training and test data. This function creates a TfidfVectorizer object and fits it on the training data. It then transforms the training and test data using this fitted vectorizer. Finally, it saves the vectorizer object to a file using pickle. The function returns the transformed training and test data."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the tagged_cve_cwe dataset\n",
        "input_fd = open('tagged_cve_cwe.txt', encoding='utf-8', errors = 'backslashreplace')\n",
        "cwe_df = pd.read_csv(input_fd, sep='\\t', engine='python').sample(4593, random_state=0)\n",
        "cwe_df.head()\n",
        "\n",
        "# Split data into features and labels\n",
        "x = cwe_df['Defect'].values\n",
        "y = cwe_df['CWE'].values\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
        "print('Train size = {}, test size = {}'.format(len(X_train), len(X_test)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Train size = 3674, test size = 919\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679617667971
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code reads in the training and test data from a CSV file, and creates training and test datasets. The train_test_split function is used to split the data into training and test sets. The function prints the size of the training and test sets."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tfidf, X_test_tfidf = tfidf_features(X_train, X_test,'./tfidf_vectorizer.pkl')\n",
        "vectorizer = pickle.load(open('./tfidf_vectorizer.pkl', 'rb'))\n",
        "X_train_tfidf, X_test_tfidf = vectorizer.transform(X_train), vectorizer.transform(X_test)"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679617670388
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we perform TF-IDF transformation on the training and test datasets. We use the tfidf_features function defined earlier to fit and transform the data. Then we load the vectorizer object saved earlier and transform the data using this saved vectorizer."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(solver='newton-cg',C=5, penalty='l2',n_jobs=-1)\n",
        "cwe_classifier = OneVsRestClassifier(lr)\n",
        "cwe_classifier.fit(X_train_tfidf, y_train)\n",
        "OneVsRestClassifier(estimator=LogisticRegression(C=5, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1, penalty='l2', random_state=None, solver='newton-cg', tol=0.0001, verbose=0, warm_start=False), n_jobs=1)\n",
        "y_test_pred = cwe_classifier.predict(X_test_tfidf)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print('Test accuracy = {}'.format(test_accuracy))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Test accuracy = 0.940152339499456\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679617673883
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section of the code trains a Logistic Regression model using the One-vs-Rest (OvR) strategy for multi-class classification. The solver used here is \"newton-cg\", which is a gradient-based optimization algorithm used to solve the logistic regression optimization problem. The parameter C is set to 5, which controls the trade-off between fitting the training data and avoiding overfitting. The penalty used is L2 regularization, which helps to avoid overfitting.\n",
        "\n",
        "The OneVsRestClassifier is a strategy that trains multiple classifiers, one for each class, and then selects the class with the highest probability. The fit method is called on the OneVsRestClassifier object using the X_train_tfidf and y_train data. This trains the classifier on the training data.\n",
        "\n",
        "Once the model is trained, the predict method is called on the cwe_classifier object using the X_test_tfidf data to generate the predicted classes for the test data. The accuracy_score method from the sklearn.metrics module is used to calculate the accuracy of the model's predictions on the test data. The accuracy is printed to the console."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def guess_cwe(cve_title):\n",
        "    # Prepare the input text\n",
        "    cve_title_prepared = text_prepare(cve_title)\n",
        "    # Transform the text into features using the pre-trained vectorizer\n",
        "    features = vectorizer.transform([cve_title_prepared])\n",
        "    # Make a prediction using the pre-trained classifier\n",
        "    prediction_prob = cwe_classifier.predict_proba(features)\n",
        "    prediction_score = max(prediction_prob[0])\n",
        "    prediction_percent = prediction_score * 100\n",
        "    # Return the predicted CWE\n",
        "    predicted_cwe = cwe_classifier.predict(features)[0]\n",
        "    print(f\"The predicted score for {cve_title} (CWE-{predicted_cwe}): {prediction_percent:.2f}%\")\n",
        "    return predicted_cwe"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679617921641
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we create a function called \"guess_cwe\" that takes the name of a vulnerability as input and outputs the predicted CWE category for that vulnerability title, based on what the machine learning model has learned previously. The function uses the trained model and the TF-IDF transformation that was learned during training to make the prediction."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "guess_cwe('Structured Query Language Injection')\n",
        "guess_cwe('SQL Injection')\n",
        "guess_cwe('Double Free')\n",
        "guess_cwe('Use After Free')\n",
        "guess_cwe('OS CMD Injection')\n",
        "guess_cwe('Denial of Service')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "The predicted score for Structured Query Language Injection (CWE-89): 40.97%\nThe predicted score for SQL Injection (CWE-89): 94.04%\nThe predicted score for Double Free (CWE-415): 94.94%\nThe predicted score for Use After Free (CWE-416): 93.36%\nThe predicted score for OS CMD Injection (CWE-78): 75.20%\nThe predicted score for Denial of Service (CWE-400): 66.35%\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 24,
          "data": {
            "text/plain": "400"
          },
          "metadata": {}
        }
      ],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679617963555
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}